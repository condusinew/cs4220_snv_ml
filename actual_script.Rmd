---
getwd(title: "ML project"
output: html_document
date: "2025-02-12"
---

```{r}
setwd("/Users/cindyyu/Desktop/CS4220")
library(readxl)
library(tidyverse)
library(caTools)
library(randomForest)
library(caret)
library(VIM)
library(ggplot2)
library(mice)
library(ranger)


library(rpart)
library(tidyr)
```

#READING IN FILTER PREDICTIONS, and MERGING THEM
```{r}
real1_df <- read.table("snv-parse-real1.txt", header = T)
real2.1_df <- read.table("snv-parse-real2_part1.txt", header = T)
syn1_df <- read.table("snv-parse-syn1.txt", header = T)
syn2_df <- read.table("snv-parse-syn2.txt", header = T)
syn3_df <- read.table("snv-parse-syn3.txt", header = T)
syn4_df <- read.table("snv-parse-syn4.txt", header = T)
syn5_df <- read.table("snv-parse-syn5.txt", header = T)

all_df <- rbind(real1_df, real2.1_df, syn1_df, syn2_df, syn3_df, syn4_df, syn5_df)
```

#READING IN TRUTHS, and MERGING THEM 
```{r}
#add headers: chromosome #, genomic start, genomic stop
real1_truth <- as.data.frame(read.table("/Users/cindyyu/Desktop/CS4220/Data/real1/real1_truth.bed",
                                header = FALSE, sep="\t",stringsAsFactors=FALSE, quote=""))
real2.1_truth <- as.data.frame(read.table("/Users/cindyyu/Desktop/CS4220/Data/real2_part1/real2_truth_chr1to5.bed",
                                header = FALSE, sep="\t",stringsAsFactors=FALSE, quote=""))
syn1_truth <- as.data.frame(read.table("/Users/cindyyu/Desktop/CS4220/Data/syn1/syn1_truth.bed",
                                header = FALSE, sep="\t",stringsAsFactors=FALSE, quote=""))
syn2_truth <- as.data.frame(read.table("/Users/cindyyu/Desktop/CS4220/Data/syn2/syn2_truth.bed",
                                header = FALSE, sep="\t",stringsAsFactors=FALSE, quote=""))
syn3_truth <- as.data.frame(read.table("/Users/cindyyu/Desktop/CS4220/Data/syn3/syn3_truth.bed",
                                header = FALSE, sep="\t",stringsAsFactors=FALSE, quote=""))
syn4_truth <- as.data.frame(read.table("/Users/cindyyu/Desktop/CS4220/Data/syn4/syn4_truth.bed",
                                header = FALSE, sep="\t",stringsAsFactors=FALSE, quote=""))
syn5_truth <- as.data.frame(read.table("/Users/cindyyu/Desktop/CS4220/Data/syn5/syn5_truth.bed",
                                header = FALSE, sep="\t",stringsAsFactors=FALSE, quote=""))
#might not need this
alltruth_df <- rbind(real1_truth, real2.1_truth, syn1_truth, syn2_truth, syn3_truth, syn4_truth, syn5_truth)

names(alltruth_df) <- c("Chr", "START_POS_REF", "END_POS_REF")

write.table(alltruth_df, "all_truths.bed",row.names = F,col.names = F, sep="\t", quote=FALSE)
```

#JOINS SETS SO THAT WE HAVE A TRUTH COL NEXT TO FEATURES
```{r}

#FOR JOINING TRUTH AND VCF FILES
jointruth<- function(truthdf, predsdf) {
  names(truthdf) <- c("Chr", "START_POS_REF", "END_POS_REF")
  truthdf$TRUTH = TRUE
  newdf <- predsdf %>% 
    left_join(truthdf, by=c('Chr','START_POS_REF', 'END_POS_REF'))
  newdf$TRUTH <- replace_na(newdf$TRUTH, FALSE)
  return(newdf)
}


#joins for vcfs and truths
df3 <- jointruth(alltruth_df, all_df)
# real1_df <- jointruth(real1_truth, real1_df)
# real2.1_df <- jointruth(real2.1_truth, real2.1_df)
# syn1_df <- jointruth(syn1_truth, syn1_df)
# syn2_df <- jointruth(syn2_truth, syn2_df)
# syn3_df <- jointruth(syn3_truth, syn3_df)
# syn4_df <- jointruth(syn4_truth, syn4_df)
# syn5_df <- jointruth(syn5_truth, syn5_df)
```

#NUMBER OF POSITIONS AND PROPORTION OF VARIANTS
```{r}
counts <- df3 |>
  group_by(Sample_Name, TRUTH) |>
  tally()

ggplot(counts, aes(fill=TRUTH, y=n, x=Sample_Name)) + 
    geom_bar(stat="identity", width = 0.7)

counts <- df3 |>
  group_by(TRUTH) |>
  tally()

ggplot(counts, aes(y=n, x=TRUTH)) + 
    geom_bar(stat="identity", width = 0.7)
```


#DATA IMPUTATION
##plotting, same group, medians
```{r}
#SHOWING WHERE TRUE FALSES ARE BY STACKED BAR PLOT
#sorry for the very inefficient code 

#MUTECT
df_m2na <- df3 |> 
  group_by(FILTER_Mutect2, is.na(m2_MQ)) |>
  tally() 

colnames(df_m2na)<- c("FILTER_Mutect2","Is_NA","n")

gg_m2na <- ggplot(df_m2na, aes(fill=Is_NA, y=n, x=FILTER_Mutect2)) + 
    geom_bar(stat="identity", width = 0.7)

#FREEBAYES
df_fbna <- df3 |> 
  group_by(FILTER_Freebayes, is.na(f_MQMR)) |>
  tally() 

colnames(df_fbna)<- c("FILTER_Freebayes","Is_NA","n")

gg_fbna <- ggplot(df_fbna, aes(fill=Is_NA, y=n, x=FILTER_Freebayes)) + 
    geom_bar(stat="identity", width = 0.7)

#VARDICT
df_vdna <- df3 |> 
  group_by(FILTER_Vardict, is.na(vd_SSF), is.na(vd_MSI)) |>
  tally() 

df_vdna <- df_vdna[-3]
colnames(df_vdna)<- c("FILTER_Vardict","Is_NA","n")

#MENTION THAT NA IN SSV ALSO HAD NA IN MSI
gg_vdna <- ggplot(df_vdna, aes(fill=Is_NA, y=n, x=FILTER_Vardict)) + 
    geom_bar(stat="identity", width = 0.7)

#VARSCAN
df_vsna <- df3 |> 
  group_by(FILTER_Varscan, is.na(vs_SPV), is.na(vs_SSC)) |>
  tally() 

df_vsna <- df_vsna[-3]
colnames(df_vsna)<- c("FILTER_Varscan","Is_NA","n")

#MENTION THAT NA IN SSV ALSO HAD NA IN MSI
gg_vsna <- ggplot(df_vsna, aes(fill=Is_NA, y=n, x=FILTER_Varscan)) + 
    geom_bar(stat="identity", width = 0.7)

df3 %>%
  ggplot(aes(x = FILTER_Mutect2, y = m2_MQ)) +
  geom_bar(stat = "identity")

```

#MICE AGGR PLOT 
```{r}

aggr_plot <- aggr(df3, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(df3), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))

```

#ACTUAL MICE IMPUTATION
```{r}
#missForest
library(missForest)

#RUN THIS EVEN IF UR RETRIEVING MICE FROM TEXT FILE
df3_features <- df3[13:19]
df3_features$TRUTH = as.factor(df3_features$TRUTH)

library(mice)
#seed 10% missing values
df3.mis <- prodNA(df3_features, noNA = 0.1)
summary(df3.mis)

imputed_Data <- mice(df3.mis, m=5, maxit = 50, method = 'pmm', seed = 500)
summary(imputed_Data)

complete_data <- complete(imputed_Data, action = "long")

#ITERATE THRU 
mice1 <- complete(imputed_Data, 1)
mice2 <- complete(imputed_Data, 2)
mice3 <- complete(imputed_Data, 3)
mice4 <- complete(imputed_Data, 4)
mice5 <- complete(imputed_Data, 5)

#so i don't have to run mice each time 
write.table(mice1, "mice1.txt",row.names = F,col.names = F, sep="\t", quote=FALSE)
write.table(mice2, "mice2.txt",row.names = F,col.names = F, sep="\t", quote=FALSE)
write.table(mice3, "mice3.txt",row.names = F,col.names = F, sep="\t", quote=FALSE)
write.table(mice4, "mice4.txt",row.names = F,col.names = F, sep="\t", quote=FALSE)
write.table(mice5, "mice5.txt",row.names = F,col.names = F, sep="\t", quote=FALSE)

#so we dont have to run impute each time
mice1 <- read.table("mice1.txt", sep="\t")
names(mice1) <- colnames(df3_features)
```

#MACHINE LEARNING SINGLE CASE IMPUTATION MEDIAN LOOK LATER
```{r, echo = skip}
#processing + split into test/train
mice1$TRUTH <- as.factor(mice1$TRUTH)
set.seed(123)
split <- sample.split(mice1$TRUTH, SplitRatio = 0.8)
training_set <- subset(mice1, split == TRUE)
test_set <- subset(mice1, split == FALSE)
training_featuresremoved <- training_set[, 13:20]

#ver1: weights inversely proportional to sample size
sample_counts <- table(mice1$Sample_Name)
weights <- max(sample_counts) / sample_counts

# Train a weighted Random Forest
rf_model_median <- randomForest(
  TRUTH ~ ., 
  data = training_featuresremoved, 
  ntree = 100, 
  strata = mice1$Sample_Name, 
  sampsize = rep(min(sample_counts), length(sample_counts)), 
  importance = TRUE
)


#actually training
rf_model_median <- randomForest(TRUTH ~ ., data = training_featuresremoved, ntree = 100, importance = TRUE)
test_predictions_median <- predict(rf_model_median, newdata = test_set) #predicts
confusion_matrix <- table(test_set$TRUTH, test_predictions_median)
print(confusion_matrix)

importance(rf_model_median)
varImpPlot(rf_model) #unsure what this does


#PREDICTIONS INTO BED
test_setnew <- test_set[,1:3] 
test_setnew$TRUTH <- test_pred

test_setnew <- test_setnew |>
  filter(TRUTH == TRUE)

test_setnew <- test_setnew[1:3]

write.table(test_set, "mediantest_predictions.bed",row.names = F,col.names = F, sep="\t", quote=FALSE)

#TEST SET INTO BED
testset_truth <- test_set[, -(4:18)] |>
  filter(TRUTH == TRUE)


write.table(testset_truth, "mediantest_TRUTH",row.names = F,col.names = F, sep="\t", quote=FALSE)
```

```{r, echo = skip}
test_set_real1 <- test_set |>
  filter(Sample_Name == "icgc_cll-T") 
test_predictions_median <- predict(rf_model_median, newdata = test_set_real1) #predicts

confusion_matrix <- table(test_set_real1$TRUTH, test_predictions_median)
print(confusion_matrix)


test_set_notruth <- test_set[,-19]
test_predictions_median <- predict(rf_model_median, newdata = test_set_notruth) #predicts

confusion_matrix <- table(test_set$TRUTH, test_predictions_median)
print(confusion_matrix)


test_set_real1 <- test_set |>
  filter(Sample_Name == "dream1-T") 
test_predictions_median <- predict(rf_model_median, newdata = test_set_real1) #predicts

confusion_matrix <- table(test_set_real1$TRUTH, test_predictions_median)
print(confusion_matrix)

```



#MACHINE LEARNING MICE
```{r}
# 
# set.seed(123)
# 
# mice1 <- read.table('impute_mice1.txt', sep = ',', col.names = c("m2_MQ", "f_MQMR", "vs_SSC", "vs_SPV", "vd_SSF", "vs_MSI", "TRUTH"))
# 
# df_mice1 <- df3
# df_mice1[13:19] <- mice1
# 
# df_mice1$TRUTH <- as.factor(df_mice1$TRUTH)
# split <- sample.split(df_mice1$TRUTH, SplitRatio = 0.8)
# 
# 
# training_set <- subset(df_mice1, split == TRUE)
# test_set <- subset(df_mice1, split == FALSE)
# 
# 
# rf_model_mice <- randomForest(TRUTH ~ ., data = training_set, ntree = 100, importance = TRUE)
# 
# test_predictions_mice <- predict(rf_model_mice, newdata = test_set_notruth)
# 
# confusion_matrix_mice <- table(test_set$TRUTH, test_predictions_mice)
# print(confusion_matrix_mice)
# 
# importance(rf_model)
# varImpPlot(rf_model)
# 
# 
# test_pred <- test_predictions
# write.table(test_pred, "test_predictions_vector.bed",row.names = F,col.names = F, sep="\t", quote=FALSE)
# 
# #test_pred <-read.table("REALtest_predictions.bed", sep="\t" )


#PREDICTIONS INTO BED
# test_setnew <- test_set[,1:3] 
# test_setnew$TRUTH <- test_pred
# 
# test_setnew <- test_setnew |>
#   filter(TRUTH == TRUE)
# 
# test_setnew <- test_setnew[1:3]
# 
# 
# write.table(test_set, "REALtest_predictions.bed",row.names = F,col.names = F, sep="\t", quote=FALSE)
# 
# #TEST SET INTO BED
# testset_truth <- test_set[, -(4:18)] |>
#   filter(TRUTH == TRUE)
# 
# 
# write.table(testset_truth, "REALtest_TRUTH.bed",row.names = F,col.names = F, sep="\t", quote=FALSE)
```

#FINDING CASE WEIGHTS
```{r}

set.seed(123)
#df3_median <- df3 %>% mutate_all(~ifelse(is.na(.x), median(.x, na.rm = TRUE), .x)) 
 

find_case_weights <- function(df){
  #case_weights <- data.frame()
  df$Sample_Name <- df3$Sample_Name
  df$TRUTH <- as.factor(df$TRUTH)
  df$Sample_Name <- as.factor(df$Sample_Name)

  #gotta be a better way to do this
  
  #weights depending on both sample and minority mutation class
  sample_counts <- table(df$Sample_Name)
  sample_counts <- as.data.frame(sample_counts)
  sample_counts$Freq <- as.numeric(sample_counts$Freq)
  sample_counts$weights <- max(sample_counts$Freq) / sample_counts$Freq
  
  mutation_counts <- table(df$TRUTH)
  mutation_counts <- as.data.frame(mutation_counts)
  
  mutation_counts$Freq <- as.numeric(mutation_counts$Freq)
  mutation_counts$weights <- max(mutation_counts$Freq) / mutation_counts$Freq

  sample_counts$case_weights <- sample_counts$weights*mutation_counts$weights

  return(sample_counts)
  
}
#every loop i make takes too long to figure out.
#diff comboes of case weight based to discretion
#im realizing this is / just . addign
inc <- function(x, w){
 return(x+w)
}

case_weights1 <- find_case_weights(mice1)
case_weights1 <- case_weights1[,-(2:3)]
names(case_weights1) <- c("Sample_Name", "Case_Weights")
case_weights2 <- case_weights1
case_weights3 <- case_weights1
case_weights4 <- case_weights1
case_weights5 <- case_weights1

#decrease real data
case_weights2[1:4, 2] <- inc(case_weights2[1:4, 2], -1)

#increase real data
case_weights3[5:6, 2] <- inc(case_weights2[5:6, 2], 1)
case_weights4[5:6, 2] <- inc(case_weights2[5:6, 2], 2)

#make synth data = 1 
case_weights5[1:4, 2] <- 1

mice1$Sample_Name <- df3$Sample_Name

mice1.1 <- mice1 %>% 
    left_join(case_weights1, by="Sample_Name")
mice1.2 <- mice1 %>% 
    left_join(case_weights2, by="Sample_Name")
mice1.3 <- mice1 %>% 
    left_join(case_weights3, by="Sample_Name")
mice1.4 <- mice1 %>% 
    left_join(case_weights4, by="Sample_Name")
mice1.5 <- mice1 %>% 
    left_join(case_weights5, by="Sample_Name")
```

#TRAINING MODEL, RANDOM FOREST, NON-WEIGHTED
```{r}
#training, just RF
rf_model_median <- randomForest(TRUTH ~ ., data = training_set_features, ntree = 100, importance = TRUE)

#predictions w RF, not weighted
test_predictions_median <- predict(rf_model_median, newdata = test_set)
confusion_matrix_median <- table(test_set$TRUTH, test_predictions_median)
print(confusion_matrix_median) #F1: 0.9926
```

#TRAINING MODEL, RANDOM FOREST, WEIGHTED
```{r}
#training weighted RF model
rf_model_median <- ranger(
  TRUTH ~ ., 
  data = training_set_features, 
  case.weights = training_set_features$Freq, 
  num.trees = 100
)

print(rf_model)

#predict w ranger
predictions <- predict(rf_model, data = test_set)
predicted_classes <- predictions$predictions

confusion_matrix_median <- table(test_set$TRUTH, predicted_classes)
print(confusion_matrix_median)
```


```{r}
importance(rf_model)
```


grid search 
```{r}
# Define the grid of hyperparameters to search
# oh we ran this previously with different params and got these best ones
# and then did further optimization closer to the 'best' #
# param_grid <- expand.grid(
#   cp = c(0.01, 0.02, 0.05, 0.1, 0.2),
#   maxdepth = c(3, 5, 7, 9, 11), 
#   minsplit = c(5, 10, 20, 30, 40), 
#   weights = c(case_weights1, case_weights2, case_weights3, case_weights4, case_weights5),
#   stringsAsFactors = FALSE
# )
# 
# # Create empty list to store results
# results <- list()
# 
# # Set up the data
# split_vals <- sample.split(mice1.1$TRUTH, SplitRatio = 0.80)
# train_set <- subset(mice1.1, split_vals == TRUE)
# test_set <- subset(mice1.1, split_vals == FALSE)
# 
# train_set_split <- train_set[, -8]  # Removing unnecessary columns
# 
# # Perform Grid Search
# for (i in 1:nrow(param_grid)) {
#   
#   # Extract hyperparameters from the grid
#   cp_value <- param_grid$cp[i]
#   maxdepth_value <- param_grid$maxdepth[i]
#   minsplit_value <- param_grid$minsplit[i]
#   weights <- param_grid$weights
#   
#   # Train the decision tree model with the current hyperparameters
#   rf_model_ <- ranger(
#     TRUTH ~ ., 
#     data = train_set_split, 
#     case.weights = train_set_split$Freq, 
#     cp = cp_value,
#     maxdepth = maxdepth_value,
#     minsplit = minsplit_value,
#     num.trees = 100
#     )
#   # 
#   # mod_class <- rpart(as.factor(TRUTH) ~ ., 
#   #                    data = train_set_split,
#   #                    cp = cp_value,
#   #                    maxdepth = maxdepth_value,
#   #                    minsplit = minsplit_value
#   #                    )
#   
#   # Make predictions on the test set
#   result_class <- predict(mod_class, test_set, type = 'class')
#   
#   # Calculate performance metric (accuracy in this case)
#   accuracy <- sum(test_set$TRUTH == result_class) / nrow(test_set)
#   
#   # Store the result
#   results[[i]] <- list(
#     cp = cp_value,
#     maxdepth = maxdepth_value,
#     minsplit = minsplit_value,
#     accuracy = accuracy
#   )
# }
# 
# # Convert results to a data frame
# results_df <- do.call(rbind, lapply(results, as.data.frame))
# 
# # Find the best hyperparameters (highest accuracy)
# best_model <- results_df[which.max(results_df$accuracy), ]
# print(best_model)
# 
# # Optional: Use the best hyperparameters to refit the model
# best_mod_class <- rpart(as.factor(TRUTH) ~ ., 
#                         data = train_set_split,
#                         cp = best_model$cp,
#                         maxdepth = best_model$maxdepth,
#                         minsplit = best_model$minsplit)
# 
# # Make predictions with the best model
# final_result_class <- predict(best_mod_class, test_set, type = 'class')
# 
# # Display confusion matrix for the best model
# table(test_set$TRUTH, final_result_class)
```
#TRAINING MODEL, RANDOM FOREST, WEIGHTED
```{r}
# Define the grid of hyperparameters to search
# oh we ran this previously with different params and got these best ones
# and then did further optimization closer to the 'best' #

mice_dfs = list(mice1.1, mice1.2, mice1.3, mice1.4, mice1.5)
corresponding_weights = list("case_weights1","case_weights2","case_weights3","case_weights4","case_weights5")

#empty df
matrix <- matrix(nrow = 1, ncol = 4)
col_names <- c("weight", "precision", "recall", "F1")
results <- data.frame(matrix = matrix)
colnames(results) <- col_names


results <- data.frame(
  "weight" = c(),
  "precision" = c(),
  "recall" = c(),
  "F1" = c()
)
 
for (i in 1:5) {
  
  # Extract hyperparameters from the grid
  df <- mice_dfs[i]
  df <- df[[1]]

  weights_value <- corresponding_weights[i]
  
  split_vals <- sample.split(df$TRUTH, SplitRatio = 0.80)
  train_set <- subset(df, split_vals == TRUE)
  test_set <- subset(df, split_vals == FALSE)

  train_set_split <- train_set[, -8]
  
  # Train the decision tree model with the current hyperparameters
  rf_model_weighted <- ranger(
    TRUTH ~ ., 
    data = train_set_split, 
    case.weights = train_set_split$Case_Weights
    )

  #predict
  result_class <- predict(rf_model_weighted, test_set)
  
  #calculate performance metric (accuracy in this case), but we should change to f1
  TP <- sum(test_set$TRUTH == result_class$predictions)
  FN <- sum(test_set$TRUTH != result_class$predictions & result_class$predictions == FALSE)
  
  
  precision <- TP/(sum(result_class$predictions == TRUE))
  recall <- sum(TP/(TP+FN))

  F1 <- (2*precision*recall)/(precision+recall)
  
  #store result
  results[i, 1] <- weights_value
  results[i, 2] <- precision
  results[i, 3] <- recall
  results[i, 4] <- F1
}

#show iterated results
results

best_model <- results[which.max(results$V4)]
print(best_model)

# # Optional: Use the best hyperparameters to refit the model
best_model_class <- ranger(
      TRUTH ~ ., 
      data = train_set_split, 
      case.weights = train_set_split$Case_Weights
    )

# Make predictions with the best model
final_result_class <- predict(best_mod_class, test_set, type = 'class')

# Display confusion matrix for the best model
table(test_set$TRUTH, final_result_class)

```

#TRAINING MODEL, RPART
```{r}
#results <- list()

param_grid <- expand.grid(
  cp = c(0.01, 0.05, 0.1),
  maxdepth = c(3, 5, 7), 
  minsplit = c(5, 10, 20), 
  stringsAsFactors = FALSE
)


# Set up the data
split_vals <- sample.split(mice1.1$TRUTH, SplitRatio = 0.80)
train_set <- subset(mice1.1, split_vals == TRUE)
test_set <- subset(mice1.1, split_vals == FALSE)

train_set_split <- train_set[, -(8:9)]  # Removing unnecessary columns


results <- list()
# Perform Grid Search
for (i in 1:nrow(param_grid)) {
  
  # Extract hyperparameters from the grid
  cp_value <- param_grid$cp[i]
  maxdepth_value <- param_grid$maxdepth[i]
  minsplit_value <- param_grid$minsplit[i]
  weights_value <- param_grid$weights
  
  # Train the decision tree model with the current hyperparameters
  mod_class <- randomForest(as.factor(TRUTH) ~ .,
                     data = train_set_split,
                     cp = cp_value,
                     maxdepth = maxdepth_value,
                     minsplit = minsplit_value
                     )

  # Make predictions on the test set
  result_class <- predict(mod_class, test_set, type = 'class')

  # Calculate performance metrics
  
  #calculate performance metric (accuracy in this case), but we should change to f1
  TP <- sum(test_set$TRUTH == result_class)
  FN <- sum(test_set$TRUTH != result_class & result_class == FALSE)
  precision <- TP/(sum(result_class == TRUE))
  recall <- sum(TP/(TP+FN))

  F1 <- (2*precision*recall)/(precision+recall)

  results[[i]] <- list(
    cp = cp_value,
    maxdepth = maxdepth_value,
    minsplit = minsplit_value,
    precision = precision,
    recall = recall,
    F1 = F1
  )
}

# Convert results to a data frame
results_df <- do.call(rbind, lapply(results, as.data.frame))

# Find the best hyperparameters (highest accuracy)
best_model <- results_df[which.max(results_df$accuracy), ]
print(best_model)

# Optional: Use the best hyperparameters to refit the model
best_mod_class <- rpart(as.factor(TRUTH) ~ ., 
                        data = train_set_split,
                        cp = best_model$cp,
                        maxdepth = best_model$maxdepth,
                        minsplit = best_model$minsplit)

# Make predictions with the best model
final_result_class <- predict(best_mod_class, test_set, type = 'class')

# Display confusion matrix for the best model
table(test_set$TRUTH, final_result_class)

```




#TRAINING MODEL, RPART
```{r}

# Create empty list to store results
library(rpart)
results <- list()

param_grid <- expand.grid(
  cp = c(0.01, 0.02, 0.05, 0.1, 0.2),
  maxdepth = c(3, 5, 7, 9, 11), 
  minsplit = c(5, 10, 20, 30, 40), 
  stringsAsFactors = FALSE
)


# Set up the data
split_vals <- sample.split(mice1.1$TRUTH, SplitRatio = 0.80)
train_set <- subset(mice1.1, split_vals == TRUE)
test_set <- subset(mice1.1, split_vals == FALSE)

train_set_split <- train_set[, -8]  # Removing unnecessary columns

# Perform Grid Search
for (i in 1:nrow(param_grid)) {
  
  # Extract hyperparameters from the grid
  cp_value <- param_grid$cp[i]
  maxdepth_value <- param_grid$maxdepth[i]
  minsplit_value <- param_grid$minsplit[i]
  weights_value <- param_grid$weights
  
  # Train the decision tree model with the current hyperparameters
  mod_class <- rpart(as.factor(TRUTH) ~ .,
                     data = train_set_split,
                     cp = cp_value,
                     maxdepth = maxdepth_value,
                     minsplit = minsplit_value
                     )

  # Make predictions on the test set
  result_class <- predict(mod_class, test_set, type = 'class')
  
  # Calculate performance metrics
  
  #calculate performance metric (accuracy in this case), but we should change to f1
  TP <- sum(test_set$TRUTH == result_class)
  FN <- sum(test_set$TRUTH != result_class & result_class == FALSE)
  
  accuracy <- TP/sum(result_class == TRUE)
  precision <- TP/(TP+FN)
  F1 <- (2*precision*recall)/(precision+recall)
  
  # Store the result
  results[[i]] <- list(
    cp = cp_value,
    maxdepth = maxdepth_value,
    minsplit = minsplit_value,
    accuracy = accuracy
    precision = precision
    F1 = F1
  )
}

# Convert results to a data frame
results_df <- do.call(rbind, lapply(results, as.data.frame))

# Find the best hyperparameters (highest accuracy)
best_model <- results_df[which.max(results_df$accuracy), ]
print(best_model)

# Optional: Use the best hyperparameters to refit the model
best_mod_class <- rpart(as.factor(TRUTH) ~ ., 
                        data = train_set_split,
                        cp = best_model$cp,
                        maxdepth = best_model$maxdepth,
                        minsplit = best_model$minsplit)

# Make predictions with the best model
final_result_class <- predict(best_mod_class, test_set, type = 'class')

# Display confusion matrix for the best model
table(test_set$TRUTH, final_result_class)
```
